{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyPm1UANoxz3UJ9VEnjACl21",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ashwathdnd/Age-Recognition-Model/blob/main/SEC_Compliance_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n3do8H3NTgh"
   },
   "outputs": [],
   "source": [
    "# --- 1. Install necessary libraries ---\n",
    "# We use ! to run shell commands in Colab\n",
    "print(\"Installing necessary libraries...\")\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U datasets\n",
    "\n",
    "print(\"Libraries installed successfully!\")\n",
    "\n",
    "# --- 2. Log in to Hugging Face ---\n",
    "# To download the Llama 3 model, you need a Hugging Face account and a token.\n",
    "# 1. Go to https://huggingface.co/settings/tokens to get your token.\n",
    "# 2. When you run this cell, a box will appear. Paste your token and press Enter.\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"Please log in to Hugging Face.\")\n",
    "login()\n",
    "\n",
    "print(\"Login successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lou82CdIO9DT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# --- This cell no longer contains TrainingArguments or pipeline ---\n",
    "\n",
    "# --- Configure Model Quantization (for efficiency) ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# --- Load the Llama 3 Model & Tokenizer ---\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(f\"\\nLoading base model: {model_id}\")\n",
    "\n",
    "# Load the model with our 4-bit configuration\n",
    "# We have REMOVED the device_map argument to let the Trainer handle it.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install the 'trl' library which contains the SFTTrainer\n",
    "!pip install -q trl"
   ],
   "metadata": {
    "id": "CXwAl5q6yl8_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- 1. Load the Dataset ---\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/training_data.jsonl\", split=\"train\")\n",
    "\n",
    "# --- 2. Preprocess the Dataset ---\n",
    "def preprocess_function(examples):\n",
    "    text = f\"### INSTRUCTION:\\nAnalyze the following text for SEC Marketing Rule compliance.\\n\\n### TEXT:\\n{examples['text']}\\n\\n### ANALYSIS:\\n{examples['explanation']}\"\n",
    "    return tokenizer(text, truncation=True, max_length=512)\n",
    "\n",
    "processed_dataset = dataset.map(preprocess_function, remove_columns=dataset.column_names)\n",
    "\n",
    "# --- 3. Configure LoRA and Prepare the Model ---\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- 4. Set Up the Training Arguments ---\n",
    "# --- THIS IS THE SECTION WE ARE FIXING ---\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # This new line disables all external logging, including wandb.\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# --- 5. Create the Trainer and Start Training! ---\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting the fine-tuning process...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete!\")\n",
    "\n",
    "# --- 6. Save the Final Model ---\n",
    "final_model_path = \"./final_model\"\n",
    "trainer.model.save_pretrained(final_model_path)\n",
    "print(f\"Final fine-tuned model saved to {final_model_path}\")"
   ],
   "metadata": {
    "id": "YIpms_xuyPyN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# The model and tokenizer should still be loaded in memory,\n",
    "# but we redefine them here for clarity in this self-contained cell.\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "final_model_path = \"./final_model\"\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, final_model_path)\n",
    "\n",
    "# --- Let's run a new test with corrected parameters ---\n",
    "\n",
    "# Test case with non-compliant language\n",
    "test_text = \"Our new 'Guaranteed Income Fund' is the safest investment on the market, providing certain returns year after year.\"\n",
    "\n",
    "# --- THIS IS THE PROMPT FIX ---\n",
    "# We make the instruction more specific to guide the model's output format.\n",
    "prompt = f\"\"\"### INSTRUCTION:\n",
    "Analyze the following text for SEC Marketing Rule compliance. Output your final analysis as a single, raw JSON object only.\n",
    "\n",
    "### TEXT:\n",
    "{test_text}\n",
    "\n",
    "### ANALYSIS:\n",
    "\"\"\"\n",
    "\n",
    "# --- THIS IS THE MAX_LENGTH FIX ---\n",
    "# We use the pipeline with a larger max_length.\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=fine_tuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512  # Increased from 200\n",
    ")\n",
    "\n",
    "result = pipe(prompt)\n",
    "\n",
    "# Print out the generated analysis\n",
    "print(\"\\n--- Model Analysis ---\")\n",
    "print(result[0]['generated_text'])"
   ],
   "metadata": {
    "id": "zYSDYiMn1EEE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- 1. Load the Base Model and Tokenizer ---\n",
    "# This loads the original Llama 3 model in 4-bit precision.\n",
    "print(\"Loading base model and tokenizer...\")\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # Let accelerate handle device mapping\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- 2. Load Your Fine-Tuned Adapter ---\n",
    "# This is where we load your specialized knowledge from the './final_model' folder\n",
    "# and apply it to the base model.\n",
    "print(\"Loading fine-tuned LoRA adapter...\")\n",
    "final_model_path = \"./final_model\"\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, final_model_path)\n",
    "\n",
    "print(\"Model ready for inference!\")\n",
    "\n",
    "\n",
    "# --- 3. Run a Test ---\n",
    "print(\"\\nRunning test inference...\")\n",
    "# This is the test case from before\n",
    "test_text = \"Our new 'Guaranteed Income Fund' is the safest investment on the market, providing certain returns year after year.\"\n",
    "\n",
    "# We format the test case into the prompt structure the model learned\n",
    "prompt = f\"\"\"### INSTRUCTION:\n",
    "Analyze the following text for SEC Marketing Rule compliance. Output your final analysis as a single, raw JSON object only.\n",
    "\n",
    "### TEXT:\n",
    "{test_text}\n",
    "\n",
    "### ANALYSIS:\n",
    "\"\"\"\n",
    "\n",
    "# Use the transformers pipeline for easy text generation\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=fine_tuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "result = pipe(prompt)\n",
    "\n",
    "# Print out the generated analysis\n",
    "print(\"\\n--- Model Analysis ---\")\n",
    "print(result[0]['generated_text'])"
   ],
   "metadata": {
    "id": "soA8p0iU3kTA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import nbformat\n",
    "\n",
    "# Replace with the name of your notebook file\n",
    "notebook_path = '/SEC Compliance Finetuning.ipynb'\n",
    "\n",
    "# Read the notebook\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Check if the problematic metadata exists and remove it\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Write the corrected notebook back to the file\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(f\"Corrected notebook saved to {notebook_path}\")"
   ],
   "metadata": {
    "id": "yDtIkywDL8NL"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
